{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatBot - LLM chatbot - have conversations - remembers previous interactions\n",
    "\n",
    "Conversational RAG -> enables a chatbot experience over a external source of data\n",
    "\n",
    "Agents -> Build a chatbot that does actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000020BCA4A2800>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000020BCA4A33D0>, model_name='gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model = \"gemma2-9b-It\", groq_api_key = groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Harish, it's nice to meet you!\\n\\nI'm glad you introduced yourself.  \\n\\nWhat can I do for you today? Are you working on a school project, need help with something, or just want to chat?  üòä\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 55, 'prompt_tokens': 21, 'total_tokens': 76, 'completion_time': 0.1, 'prompt_time': 0.00013669, 'queue_time': 0.020749787000000002, 'total_time': 0.10013669}, 'model_name': 'gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-bc56b16c-1d37-4f64-b3d4-a8b698ae2ced-0', usage_metadata={'input_tokens': 21, 'output_tokens': 55, 'total_tokens': 76})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "result = model.invoke([HumanMessage(content = \"Hi, my name is Harish and I am a student\")])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"That's a fun question!  \\n\\nSince you told me you're a student, that's what you are!  You're a learner, someone who is gaining knowledge and skills. \\n\\nIs there anything else you'd like to explore about yourself or the world around you? I'm ready for more questions! üòÑ \\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 74, 'prompt_tokens': 91, 'total_tokens': 165, 'completion_time': 0.134545455, 'prompt_time': 0.003443509, 'queue_time': 0.020153338, 'total_time': 0.137988964}, 'model_name': 'gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-5302503c-8afc-420a-8e80-d43373bf4b72-0', usage_metadata={'input_tokens': 91, 'output_tokens': 74, 'total_tokens': 165})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "model.invoke([\n",
    "    HumanMessage(content = \"Hi, my name is Harish and I am a student\"),\n",
    "    AIMessage(content=\"Hi Harish, it's nice to meet you!  \\n\\nWhat can I help you with today? Are you working on a project, studying for a test, or just looking to chat?  \\n\\nI'm here to assist in any way I can. üòä\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 58, 'prompt_tokens': 21, 'total_tokens': 79, 'completion_time': 0.105454545, 'prompt_time': 0.000140969, 'queue_time': 0.019906550999999998, 'total_time': 0.105595514}, 'model_name': 'gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-13ef5b17-7096-48bf-9ca0-266ab57dbea4-0', usage_metadata={'input_tokens': 21, 'output_tokens': 58, 'total_tokens': 79}),\n",
    "   HumanMessage(content = \"What am I?\")\n",
    "])\n",
    "# chatbots can remember the previous messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message History\n",
    "we can make the message history class to wrap our model and make it stateful. This will keep track of our inputs and outputs and store it in some DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory # very important\n",
    "\n",
    "store = {}\n",
    "\n",
    "# session id distinguishes the different sessions\n",
    "def get_session_history(session_id : str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# using this RunnableWithMessageHistory to interact with LLM model based on the chat History\n",
    "with_message_history = RunnableWithMessageHistory(model, get_session_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\" : {\"session_id\" : \"chat_1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Harish,\\n\\nIt's nice to meet you!\\n\\nWhat are you studying?  Are you working on any interesting projects right now?\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content = \"Hi my name is Harish, I am a student\")],\n",
    "    config= config\n",
    "    )\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's a fun question!  \\n\\nSince you told me your name is Harish and that you are a student, I can say that you are:\\n\\n* **A person:**  You have a name and you identify as a student.\\n* **A learner:**  Being a student means you are actively seeking knowledge and understanding.\\n* **A future something:**  What will you become after you finish your studies? A doctor? An engineer? An artist? The possibilities are endless!\\n\\n\\nIs there anything else you want to tell me about yourself? I'm curious to learn more!\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## change the config\n",
    "config = {\"configurable\" : {\"session_id\" : \"chat_1\"}}\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content = \"what am I?\")],\n",
    "    config= config\n",
    "    )\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\",\"you are helpful assistant. answer all the questions to the best of your ability\"),\n",
    "     MessagesPlaceholder(variable_name = \"messages\")] # whatever human message we give it should be key value pairs, thats why we have variable_names as messages which is a key\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Harish! It's nice to meet you. \\n\\nI'm ready to answer your questions to the best of my ability. Just ask away! üòä  \\n\\nWhat can I help you with today? \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 29, 'total_tokens': 78, 'completion_time': 0.089090909, 'prompt_time': 0.000135569, 'queue_time': 0.019914329, 'total_time': 0.089226478}, 'model_name': 'gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-808d458d-6d7a-4955-acd8-2a184cc6e7f5-0', usage_metadata={'input_tokens': 29, 'output_tokens': 49, 'total_tokens': 78})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\" : [HumanMessage(content = \"My name is harish\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(chain, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\" : {\"session_id\" : \"chat_3\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Harish, it's nice to meet you!\\n\\nI'm glad to hear you like bananas. They're a great source of potassium and energy. \\n\\nWhat can I help you with today?  Do you have any questions about bananas, or something else entirely? üòä \\n\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content = \"Hi my name is Harish, I am a student and I like Bananas\")],\n",
    "    config= config\n",
    "    )\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are a student, Harish, and you like bananas!  üçå  \\n\\nIs there anything else you'd like to know or talk about?  \\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## change the config\n",
    "config = {\"configurable\" : {\"session_id\" : \"chat_3\"}}\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content = \"what am I and which fruit I like?\")],\n",
    "    config= config\n",
    "    )\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increasing the complexity of the prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "\n",
    "    [\n",
    "        (\"system\",\"you are helpful assistant. answer all the questions to the best of your ability in {language}\"),\n",
    "        MessagesPlaceholder(variable_name = \"messages\")\n",
    "     ] # whatever human message we give it should be key value pairs, thats why we have variable_names as messages which is a key\n",
    "\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='¬°Hola! Estoy muy bien, gracias. ¬øY t√∫?  \\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 30, 'total_tokens': 47, 'completion_time': 0.030909091, 'prompt_time': 0.000166379, 'queue_time': 0.019697789, 'total_time': 0.03107547}, 'model_name': 'gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-2079a16d-c6fe-4258-8be5-77f95b967080-0', usage_metadata={'input_tokens': 30, 'output_tokens': 17, 'total_tokens': 47})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"messages\" : [HumanMessage(content = \"Hello How are you\")], \"language\" : \"spanish\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increasing the complexity of the prompt\n",
    "# \n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key = \"messages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\" : {\"session_id\" : \"chat_4\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'„Åì„Çì„Å´„Å°„ÅØÔºÅÂÖÉÊ∞ó„Åß„Åô„ÅãÔºüüòä \\n(Konnichiwa! Genki desu ka?)\\n\\nThis translates to \"Hello! How are you?\" in Japanese. \\n\\n\\nLet me know if you have any other questions!  \\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\" : [HumanMessage(content = \"hello how are you?\")],\n",
    "        \"language\" : \"japanese\"\n",
    "    },\n",
    "    config = config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing the conversation history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim_messages helps to reduce how many messages we re sending to the model\n",
    "# it allows us to specify how many tokens we want to keep, along with other parameters\n",
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "trim_message = trim_messages(\n",
    "    max_tokens = 50,\n",
    "    strategy = \"last\",\n",
    "    token_counter = model, \n",
    "    include_system = True, # including the system message\n",
    "    allow_partial = False ,# no need of partial infos\n",
    "    start_on = \"human\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You re a very good assistant', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Nothing.. help me with my project', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='sure', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Its about implementing the chatbot using langchain', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Oh thats great! In which phase can I help you?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content = \"You re a very good assistant\"),\n",
    "    HumanMessage(content = \"I am harish and I like vanilla Icecream\"),\n",
    "    AIMessage(content = \"that sounds delicious\"),\n",
    "    HumanMessage(content = \"Hello\"),\n",
    "    AIMessage(content = \"Hi! how may I help you?\"),\n",
    "    HumanMessage(content = \"Nothing.. help me with my project\"),\n",
    "    AIMessage(content = \"sure\"),\n",
    "    HumanMessage(content = \"Its about implementing the chatbot using langchain\"),\n",
    "    AIMessage(content = \"Oh thats great! In which phase can I help you?\")\n",
    "\n",
    "]\n",
    "\n",
    "trim_message.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You're working on a project to implement a chatbot using LangChain!  \\n\\nTo give you the best help, tell me more about what you're trying to achieve. For example:\\n\\n* **What kind of chatbot are you building?** (e.g., customer service, educational, creative writing assistant)\\n* **What data will your chatbot use?** (e.g., a specific dataset, access to the internet)\\n* **What specific challenges are you facing?** (e.g., choosing the right model, structuring your data, integrating with a platform)\\n\\n\\nThe more details you provide, the more tailored and helpful my assistance can be. üòÑ  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 143, 'prompt_tokens': 69, 'total_tokens': 212, 'completion_time': 0.26, 'prompt_time': 0.002037684, 'queue_time': 0.019923362, 'total_time': 0.262037684}, 'model_name': 'gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-f14aa9e6-b7e4-4459-8fb3-0508d8ac7a73-0', usage_metadata={'input_tokens': 69, 'output_tokens': 143, 'total_tokens': 212})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages = itemgetter(\"messages\") | trim_message) |\n",
    "    prompt |\n",
    "    model \n",
    ")\n",
    "chain.invoke(\n",
    "    {\"messages\" : messages + [HumanMessage(content = \"What Project am I doing?\")],\n",
    "     \"language\" : \"english\"}\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As a helpful assistant, I don't have access to your personal preferences like your favorite ice cream flavor. \\n\\nTo figure out what ice cream you like, maybe think about:\\n\\n* **Your favorite fruits:** Do you like berry flavors, chocolate, or something tropical?\\n* **Your favorite desserts:** Do you prefer rich and creamy flavors, or something lighter and tangy?\\n* **Your favorite candies:**  Do you like peanut butter, caramel, or cookie dough?\\n\\n\\n\\nLet me know if you want to brainstorm some ideas based on your likes!üç¶üòä \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 121, 'prompt_tokens': 70, 'total_tokens': 191, 'completion_time': 0.22, 'prompt_time': 0.002368223, 'queue_time': 0.027426883, 'total_time': 0.222368223}, 'model_name': 'gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-a15ac46a-4f95-4d30-bee4-67c1e8dbcf38-0', usage_metadata={'input_tokens': 70, 'output_tokens': 121, 'total_tokens': 191})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\"messages\" : messages + [HumanMessage(content = \"What Icecream do I like?\")],\n",
    "     \"language\" : \"english\"}\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapping this in the message history\n",
    "config = {\"configurable\" : {\"session_id\" : \"chat_5\"}}\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key = \"messages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm doing well, thank you.  \\n\\nHow can I help you today? üòä  \\n\\n\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\" : [HumanMessage(content = \"hello how are you?\")],\n",
    "        \"language\" : \"English\"\n",
    "    },\n",
    "    config = config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's great! Ice cream is delicious. \\n\\nWhat's your favorite flavor?  üç¶üòä  \\n\\n\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\" : [HumanMessage(content = \"I like icecreams\")],\n",
    "        \"language\" : \"english\"\n",
    "    },\n",
    "    config = config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'd love to!  \\n\\nTo help me understand how to best assist you, could you please tell me what you need help with? \\n\\nFor example, are you:\\n\\n* **Looking for information** on a specific topic?\\n* **Trying to solve a problem**?\\n* **Needing creative ideas** for something?\\n* **Just wanting to chat**?\\n\\n\\nThe more details you give me, the better I can help! üòä  \\n\\n\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\" : [HumanMessage(content = \"help me?\")],\n",
    "        \"language\" : \"english\"\n",
    "    },\n",
    "    config = config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I have no access to your past interactions, preferences, or personal information.  Therefore, I can't know what you like! \\n\\nTo help me understand your interests, tell me:\\n\\n* **What are your hobbies?**\\n* **What kind of books, movies, or music do you enjoy?**\\n* **What do you like to do in your free time?**\\n* **Is there anything you're passionate about?**\\n\\n\\nThe more you tell me, the better I can get to know you and your likes! üòä \\n\\n\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\" : [HumanMessage(content = \"what do I like?\")],\n",
    "        \"language\" : \"english\"\n",
    "    },\n",
    "    config = config\n",
    ")\n",
    "response.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
